{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.2 |Anaconda, Inc.| (default, Sep 30 2017, 18:42:57) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from IPython.display import display, Math, Latex\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf,col\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe dans les banques une équipe complète qui s'occupe de vérifier ces erreurs et de relancer ces calculs pendant la journée afin de minimiser les dégâts. Les membres de cette équipe essayent chaque jour de limiter les défaillances des énormes calculateurs. Cependant, vu l'énorme volume de données, toujours en augmentation, ces opérateurs ont une charge très importante.  \n",
    "\n",
    "\n",
    "Chaque jour les membres de cette équipe reçoivent un jeu de données qui contient des centaines de millions de lignes. Chaque ligne correspond à un risque calculé par rapport à un scénario et une maturité. Ce risque est recalculé tous les jours en fonction des données de marché et de plusieurs autres facteurs. Cependant pour détecter ces erreurs, les opérateurs sont contraints à faire des agrégations sur les données et grâce à ces agrégations, ils essayent d'estimer si les résultats sont bons ou pas et relancent tout un scop de données, s'ils doutent de son résultat.\n",
    "\n",
    "On n'a aucune indication dur les données pour savoir si une ligne (maturité d'un deal) correspond à une anomalie ou pas. Le seul indicateur qui nous permet d'identifier si le deal est une dernière version (correcte) ou une version intermédiaire (anomalie) c'est une colonne technique nommée $\\textbf{ENDDATE}$. Chaque version du deal est caractérisée par une STARTDATE et une ENDDATE. la STARTDATE indique l'heur à laquelle les calculs de risque sur le deal ont été lancés. La ENDATE peut prendre deux valeurs:\n",
    "\n",
    "- ENDDATE $\\textbf{=}$ NULL : Dernière version du deal (version correcte).\n",
    "- ENDDATE $\\textbf{!=}$ NULL : Version intermédiaire (anomalie). La valeur de la ENDDATE de cette dernière sera égale à la valeur de la STARTDATE de la version suivante.\n",
    "\t\n",
    "Ci-dessous on peut voir un résumé des étapes par lesquelles passe un deal dans une journée:\n",
    "\n",
    "<img src=\"DEAL.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def date_time_range(start, end, format , prop):\n",
    "\n",
    "    stime = time.mktime(time.strptime(start, format))\n",
    "    etime = time.mktime(time.strptime(end, format))\n",
    "\n",
    "    ptime = stime + prop * (etime - stime)\n",
    "\n",
    "    return time.strftime(format, time.localtime(ptime))\n",
    "\n",
    "def time_data(start_date_data,end_date_data):\n",
    "    start_date = date_time_range(start_date_data,end_date_data,'%d/%m/%Y %I:%M %p', random.random())\n",
    "    asofDate = start_date[0:10]\n",
    "    rand = random.random()\n",
    "    if rand > 0.5:\n",
    "        end_date = None\n",
    "    else:\n",
    "        end_date = date_time_range(start_date, asofDate + \" 11:59 PM\",'%d/%m/%Y %I:%M %p', random.random())\n",
    "    return asofDate,start_date,end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+-------------------+-------------------+-----------+\n",
      "|  ASOFDATE|KEY|MATURITY|         START_DATE|           END_DATE|RISK_AMOUNT|\n",
      "+----------+---+--------+-------------------+-------------------+-----------+\n",
      "|14/12/2016| 22|      1Y|14/12/2016 04:36 PM|14/12/2016 10:50 PM|          6|\n",
      "|16/12/2016| 31|   SEP18|16/12/2016 01:43 AM|16/12/2016 08:59 PM|          3|\n",
      "|04/12/2016| 22|      2M|04/12/2016 08:58 PM|04/12/2016 09:32 PM|          9|\n",
      "|23/12/2016| 86|   MAR18|23/12/2016 07:59 PM|23/12/2016 09:04 PM|         68|\n",
      "|14/12/2016| 58|   SEP18|14/12/2016 01:42 AM|14/12/2016 05:35 AM|         14|\n",
      "|12/12/2016| 35|      2Y|12/12/2016 08:47 AM|               null|          0|\n",
      "|16/12/2016| 75|      1Y|16/12/2016 10:49 AM|16/12/2016 12:30 PM|         25|\n",
      "|08/12/2016| 18|   MAR18|08/12/2016 08:30 PM|               null|         57|\n",
      "|04/12/2016| 27|      3M|04/12/2016 08:52 PM|04/12/2016 11:35 PM|          7|\n",
      "|20/12/2016| 43|   SEP18|20/12/2016 04:44 AM|20/12/2016 03:17 PM|         22|\n",
      "|13/12/2016| 61|   DEC17|13/12/2016 04:32 PM|13/12/2016 11:35 PM|         36|\n",
      "|01/12/2016| 91|      1Y|01/12/2016 08:38 AM|               null|         90|\n",
      "|18/12/2016| 28|      2Y|18/12/2016 02:06 PM|18/12/2016 06:46 PM|         98|\n",
      "|05/12/2016| 87|      2Y|05/12/2016 11:42 PM|               null|          0|\n",
      "|02/12/2016| 71|   DEC17|02/12/2016 04:17 PM|02/12/2016 08:34 PM|          7|\n",
      "|12/12/2016| 24|   DEC17|12/12/2016 07:38 PM|12/12/2016 09:04 PM|         14|\n",
      "|13/12/2016|  3|      2M|13/12/2016 05:15 PM|               null|         28|\n",
      "|10/12/2016| 19|   SEP18|10/12/2016 03:32 PM|               null|         72|\n",
      "|22/12/2016| 44|      2M|22/12/2016 05:50 AM|22/12/2016 04:32 PM|         51|\n",
      "|06/12/2016| 51|   DEC17|06/12/2016 05:05 AM|               null|         23|\n",
      "+----------+---+--------+-------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Input : N_data : nombre de données à générer\n",
    "        start_date_data : date du début des données générées\n",
    "        end_date_data : date de fin des données générées\n",
    "output : données d'analyse de risque généré aléatoirement\n",
    "\"\"\"\n",
    "def generate_market_risk_data(N_data,start_date_data,end_date_data):\n",
    "    random_Data = pd.DataFrame(np.random.randint(0,100,size=(N_data, 2)),\\\n",
    "                    columns=[\"KEY\",\"RISK_AMOUNT\"])\n",
    "    market_risk_data = sqlContext.createDataFrame(random_Data)\n",
    "# Genrate random Maturity\n",
    "    maturity = [\"1Y\",\"2Y\",\"3M\",\"2M\",\"MAR18\",\"SEP18\",\"DEC17\"]\n",
    "\n",
    "    rdd = market_risk_data.rdd.map(lambda x: (x['KEY'], random.choice(maturity),\\\n",
    "                                          time_data(start_date_data,end_date_data), x['RISK_AMOUNT']))\\\n",
    "                    .map(lambda x: (x[2][0],x[0],x[1],x[2][1],x[2][2],x[3]))\n",
    "    market_risk_data = rdd.toDF(['ASOFDATE','KEY','MATURITY', 'START_DATE','END_DATE','RISK_AMOUNT'])\n",
    "    market_risk_data.show()\n",
    "    return market_risk_data\n",
    "\n",
    "risk_data = generate_market_risk_data(10000,\"30/11/2016 1:30 PM\", \"27/12/2016 11:59 PM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données traitées sont sous le format $\\textbf{spark DataFrame}$ qui est une table structurée en forme de tableau avec en plus des metadatas permettant à spark de faire des optimisations sur le traitement des données. Cette table ne contient pas d'index. Elle ressemble à une table SQL avec un système de (clé, valeur). La clé d'un deal est constitué par plusieurs colonnes. Pour simplifier la compréhension des données, on va considérer qu'un deal a une seule colonne comme clé et que cette clé est unique.\n",
    "\n",
    "La variation par rapport au bon résultat de la journée peut être égale à 0 pour des versions intermédiaires. Ce qui signifie que la relance était inutile et la même donnée se trouvent à la fois sur la version relancée (par erreur) et la bonne version du jour. C'est grâce à un seuil fixé sur cette colonne qu'on pourra $\\textbf{labéliser}$ les anomalies dans le data set.\n",
    "\n",
    "Cette colonne permet aussi d'éliminer les réplications de la même donnée (ou sensiblement proches). \n",
    "\n",
    "Grâce à la colonne rajouté le problème s'est transformé d'un problème de machine learning non superviser à un problème supervisé. l'énorme volumétrie des données incite à la recherche d'une solution simple d'un point de vue calcule afin de classifier les deals en deals contenant des anomalies et des deals corrects. Cette solution doit être adapté pour recevoir les informations du deal le matin et prédire si elles sont bonnes ou pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input : données de risque de marché\n",
    "output : données avec un colonne permettant de renseigné sur la variation d'une version intermédiaire\n",
    "         par rapport au résultat final de la journée\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import broadcast,lit\n",
    "\n",
    "def variation(y,x): \n",
    "    if x != 0:\n",
    "        return (y-x)/x\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "risk_variation = udf(variation, FloatType())\n",
    "\n",
    "def add_Risk_Variation(risk_data):\n",
    "    last_versions = risk_data.filter(risk_data.END_DATE.isNull())\n",
    "    intermediate = risk_data.filter(risk_data.END_DATE.isNotNull())\n",
    "    last_versions_copy = last_versions.select('ASOFDATE','RISK_AMOUNT','KEY','MATURITY')\\\n",
    "                                             .withColumnRenamed(\"RISK_AMOUNT\",\"LAST_RISK_AMOUNT\")\\\n",
    "                                             .withColumnRenamed(\"KEY\",\"KEY2\")\\\n",
    "                                             .withColumnRenamed(\"ASOFDATE\",\"ASOFDATE2\")\\\n",
    "                                             .withColumnRenamed(\"MATURITY\",\"MATURITY2\")\n",
    "    joined_intermediate_last_version = intermediate.join(last_versions_copy, (intermediate.KEY == last_versions_copy.KEY2)&\\\n",
    "                                                        (intermediate.ASOFDATE == last_versions_copy.ASOFDATE2)&\\\n",
    "                                                        (intermediate.MATURITY == last_versions_copy.MATURITY2))\n",
    "    data_with_variation = joined_intermediate_last_version.withColumn(\"VARIATION_AMOUNT\",\\\n",
    "                    risk_variation(joined_intermediate_last_version.RISK_AMOUNT,joined_intermediate_last_version.LAST_RISK_AMOUNT))\n",
    "    last_versions = last_versions.withColumn(\"Variation_Amount\", lit(None).cast(FloatType()))\\\n",
    "                                 .withColumn(\"LAST_RISK_AMOUNT\", lit(None).cast(FloatType()))\n",
    "    drop_list = ['KEY2', 'ASOFDATE2','MATURITY2']\n",
    "\n",
    "    data_with_variation = data_with_variation.select([column for column in data_with_variation.columns if column not in drop_list])\n",
    "    Final_Frame = last_versions.unionAll(data_with_variation) \n",
    "    Final_Frame.show()\n",
    "    return Final_Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+-------------------+--------+-----------+----------------+----------------+\n",
      "|  ASOFDATE|KEY|MATURITY|         START_DATE|END_DATE|RISK_AMOUNT|Variation_Amount|LAST_RISK_AMOUNT|\n",
      "+----------+---+--------+-------------------+--------+-----------+----------------+----------------+\n",
      "|12/12/2016| 35|      2Y|12/12/2016 08:47 AM|    null|          0|            null|            null|\n",
      "|08/12/2016| 18|   MAR18|08/12/2016 08:30 PM|    null|         57|            null|            null|\n",
      "|01/12/2016| 91|      1Y|01/12/2016 08:38 AM|    null|         90|            null|            null|\n",
      "|05/12/2016| 87|      2Y|05/12/2016 11:42 PM|    null|          0|            null|            null|\n",
      "|13/12/2016|  3|      2M|13/12/2016 05:15 PM|    null|         28|            null|            null|\n",
      "|10/12/2016| 19|   SEP18|10/12/2016 03:32 PM|    null|         72|            null|            null|\n",
      "|06/12/2016| 51|   DEC17|06/12/2016 05:05 AM|    null|         23|            null|            null|\n",
      "|26/12/2016|  2|      2Y|26/12/2016 09:19 PM|    null|         45|            null|            null|\n",
      "|21/12/2016| 67|   DEC17|21/12/2016 09:25 PM|    null|         98|            null|            null|\n",
      "|07/12/2016| 86|   SEP18|07/12/2016 03:37 PM|    null|         28|            null|            null|\n",
      "|20/12/2016| 28|   DEC17|20/12/2016 03:31 PM|    null|          6|            null|            null|\n",
      "|09/12/2016| 88|      3M|09/12/2016 10:00 PM|    null|         87|            null|            null|\n",
      "|15/12/2016| 44|      1Y|15/12/2016 03:39 AM|    null|         26|            null|            null|\n",
      "|02/12/2016| 61|   SEP18|02/12/2016 04:06 AM|    null|         23|            null|            null|\n",
      "|10/12/2016| 10|   SEP18|10/12/2016 07:14 PM|    null|         97|            null|            null|\n",
      "|03/12/2016|  8|   MAR18|03/12/2016 06:58 PM|    null|         87|            null|            null|\n",
      "|20/12/2016| 74|      2M|20/12/2016 11:16 PM|    null|         42|            null|            null|\n",
      "|05/12/2016| 34|   SEP18|05/12/2016 04:45 AM|    null|         97|            null|            null|\n",
      "|20/12/2016| 23|   MAR18|20/12/2016 04:38 PM|    null|         46|            null|            null|\n",
      "|02/12/2016| 26|   DEC17|02/12/2016 06:03 AM|    null|         34|            null|            null|\n",
      "+----------+---+--------+-------------------+--------+-----------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched_market_risk_data = add_Risk_Variation(risk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input : données d'analyse de risque (chaque ligne représente un rique par rapport à une maturité)\n",
    "output: trasposition des données (maturité en colonne)\n",
    "\"\"\"\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "def pivot_data(data):\n",
    "    # add column for thresholding\n",
    "    add_minimum_variation = data.groupBy([\"ASOFDATE\",\"KEY\",\"START_DATE\"]).agg(func.min((data.Variation_Amount))\\\n",
    "                    .alias(\"MINIMUM_VARIATION\"))\n",
    "    # Pivot data\n",
    "    pivot_maturities = data.groupBy([\"ASOFDATE\",\"KEY\",\"START_DATE\"]).pivot(\"MATURITY\").agg(func.first(\"RISK_AMOUNT\"))\n",
    "    pivot_maturities = pivot_maturities.withColumnRenamed(\"ASOFDATE\",\"ASOFDATE1\")\\\n",
    "                                        .withColumnRenamed(\"START_DATE\",\"START_DATE1\")\\\n",
    "                                        .withColumnRenamed(\"KEY\",\"KEY1\")\\\n",
    "                                        .na.fill(0)\n",
    "    \n",
    "    pivoted_data = add_minimum_variation.join(pivot_maturities, (add_minimum_variation.ASOFDATE == pivot_maturities.ASOFDATE1)&\\\n",
    "                                             (add_minimum_variation.KEY == pivot_maturities.KEY1)&\\\n",
    "                                             (add_minimum_variation.START_DATE == pivot_maturities.START_DATE1))\n",
    "    drop_list = ['KEY1', 'ASOFDATE1','MATURITY1','START_DATE1']\n",
    "\n",
    "    pivoted_data = pivoted_data.select([column for column in pivoted_data.columns if column not in drop_list])\n",
    "    return pivoted_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+\n",
      "|  ASOFDATE|KEY|         START_DATE|MINIMUM_VARIATION| 1Y| 2M| 2Y| 3M|DEC17|MAR18|SEP18|\n",
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+\n",
      "|26/12/2016| 50|26/12/2016 07:10 AM|             null|  0|  0|  0|  0|    0|   48|    0|\n",
      "|04/12/2016| 58|04/12/2016 09:11 PM|             52.0|  0|  0|  0|  0|   98|    0|    0|\n",
      "|04/12/2016| 88|04/12/2016 03:01 PM|             null|  0| 73|  0|  0|    0|    0|    0|\n",
      "|19/12/2016| 11|19/12/2016 08:21 PM|             null|  0|  0|  0|  0|   71|    0|    0|\n",
      "|05/12/2016| 86|05/12/2016 03:06 PM|             null|  0|  0|  0|  0|    0|   55|    0|\n",
      "|10/12/2016| 69|10/12/2016 10:16 PM|             null|  0|  0|  0|  0|   30|    0|    0|\n",
      "|26/12/2016| 33|26/12/2016 09:28 AM|             22.0|  0|  0|  0|  0|    0|    0|   87|\n",
      "|26/12/2016| 57|26/12/2016 08:33 AM|             null|  0|  0|  0|  0|    0|   87|    0|\n",
      "|13/12/2016| 14|13/12/2016 05:56 PM|             null|  0| 96|  0|  0|    0|    0|    0|\n",
      "|08/12/2016| 43|08/12/2016 10:49 PM|             null|  0|  0|  0|  0|   61|    0|    0|\n",
      "|15/12/2016| 16|15/12/2016 12:48 AM|             null|  0|  0|  0| 55|    0|    0|    0|\n",
      "|01/12/2016| 16|01/12/2016 08:00 AM|             null| 13|  0|  0|  0|    0|    0|    0|\n",
      "|09/12/2016| 73|09/12/2016 09:54 PM|             12.0|  0|  0|  0|  0|    0|    0|    0|\n",
      "|11/12/2016| 44|11/12/2016 07:26 PM|             null| 71|  0|  0|  0|    0|    0|    0|\n",
      "|27/12/2016| 95|27/12/2016 01:56 AM|             null|  0| 25|  0|  0|    0|    0|    0|\n",
      "|19/12/2016| 91|19/12/2016 10:14 AM|             null|  0| 10|  0|  0|    0|    0|    0|\n",
      "|22/12/2016| 53|22/12/2016 11:26 AM|             92.0|  0|  0|  0| 71|    0|    0|    0|\n",
      "|21/12/2016| 31|21/12/2016 11:07 PM|             null|  0|  0|  0| 34|    0|    0|    0|\n",
      "|23/12/2016| 87|23/12/2016 10:23 AM|             null| 56|  0|  0|  0|    0|    0|    0|\n",
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_data = pivot_data(enriched_market_risk_data)\n",
    "pivoted_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+----------+------------+------------+------------+------------+---------------+---------------+---------------+\n",
      "|  ASOFDATE|KEY|         START_DATE|MINIMUM_VARIATION| 1Y| 2M| 2Y| 3M|DEC17|MAR18|SEP18|index_date|variation_1Y|variation_2M|variation_2Y|variation_3M|variation_DEC17|variation_MAR18|variation_SEP18|\n",
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+----------+------------+------------+------------+------------+---------------+---------------+---------------+\n",
      "|20/12/2016|  9|20/12/2016 06:17 AM|             null|  0|  0|  0| 31|    0|    0|    0|        19|        null|        null|        -1.0|        null|           null|           null|           null|\n",
      "|20/12/2016|  9|20/12/2016 06:17 AM|             null|  0|  0|  0| 31|    0|    0|    0|        19|        null|        null|        null|        null|           -1.0|           null|           null|\n",
      "|20/12/2016| 26|20/12/2016 06:17 AM|             null|  0|  0|  0| 75|    0|    0|    0|        19|        null|        null|        -1.0|        null|           null|           null|           null|\n",
      "|20/12/2016| 26|20/12/2016 06:17 AM|             null|  0|  0|  0| 75|    0|    0|    0|        19|        null|        null|        null|        null|           -1.0|           null|           null|\n",
      "|10/12/2016|  3|10/12/2016 09:50 PM|             null|  0|  0| 32|  0|    0|    0|    0|         9|        -1.0|        null|        null|        null|           null|           null|           null|\n",
      "|10/12/2016| 15|10/12/2016 11:43 PM|             null|  0| 60|  0|  0|    0|    0|    0|         9|        -1.0|        null|        null|        null|           null|           null|           null|\n",
      "|18/12/2016| 83|18/12/2016 04:01 PM|             null|  0| 90|  0|  0|    0|    0|    0|        17|        null|        null|        null|        null|           null|           null|           -1.0|\n",
      "|18/12/2016| 83|18/12/2016 04:01 PM|             null|  0| 90|  0|  0|    0|    0|    0|        17|        null|-0.072164945|        null|        null|           null|           null|           null|\n",
      "|18/12/2016| 78|18/12/2016 08:00 AM|             null|  0|  0|  0|  0|    2|    0|    0|        17|        null|        null|        null|        null|           null|           null|           -1.0|\n",
      "|18/12/2016| 78|18/12/2016 08:00 AM|             null|  0|  0|  0|  0|    2|    0|    0|        17|        null|        -1.0|        null|        null|           null|           null|           null|\n",
      "|13/12/2016| 23|13/12/2016 01:59 PM|             null| 18|  0|  0|  0|    0|    0|    0|        12|         0.2|        null|        null|        null|           null|           null|           null|\n",
      "|13/12/2016| 23|13/12/2016 01:59 PM|             null| 18|  0|  0|  0|    0|    0|    0|        12|      -0.625|        null|        null|        null|           null|           null|           null|\n",
      "|13/12/2016| 80|13/12/2016 12:27 AM|             null|  0|  0| 16|  0|    0|    0|    0|        12|        -1.0|        null|        null|        null|           null|           null|           null|\n",
      "|13/12/2016| 80|13/12/2016 12:27 AM|             null|  0|  0| 16|  0|    0|    0|    0|        12|        -1.0|        null|        null|        null|           null|           null|           null|\n",
      "|19/12/2016|  5|19/12/2016 05:42 AM|             44.0|  0|  0|  0| 88|    0|    0|    0|        18|        null|        null|        null|   1.8387097|           null|           null|           null|\n",
      "|19/12/2016|  5|19/12/2016 05:42 AM|             44.0|  0|  0|  0| 88|    0|    0|    0|        18|        null|        null|        null|  0.17333333|           null|           null|           null|\n",
      "|19/12/2016| 58|19/12/2016 09:07 AM|             null|  0|  0|  0|  0|    0|    0|   11|        18|        null|        null|        null|        -1.0|           null|           null|           null|\n",
      "|19/12/2016| 58|19/12/2016 09:07 AM|             null|  0|  0|  0|  0|    0|    0|   11|        18|        null|        null|        null|        -1.0|           null|           null|           null|\n",
      "|19/12/2016| 76|19/12/2016 11:01 PM|             null|  0| 97|  0|  0|    0|    0|    0|        18|        null|        null|        null|        -1.0|           null|           null|           null|\n",
      "|19/12/2016| 76|19/12/2016 11:01 PM|             null|  0| 97|  0|  0|    0|    0|    0|        18|        null|        null|        null|        -1.0|           null|           null|           null|\n",
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+----------+------------+------------+------------+------------+---------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Input : donnée d'analyse de risque de marché pivoté\n",
    "Output: donnée enréchie par la variation de chacune des maturité par rapport à la valeur du jour\n",
    "        ouvret précédent\n",
    "\"\"\"\n",
    "\n",
    "def variation2(y,x): \n",
    "    if (x != 0) & (x != None):\n",
    "        return (y-x)/x\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "day_variation = udf(variation2, FloatType())\n",
    "\n",
    "def day_to_day_variation(data):\n",
    "    maturities_list = [col for col in data.columns if col not in [\"KEY\",\"ASOFDATE\",\"START_DATE\",\"MINIMUM_VARIATION\"]]\n",
    "    asofDates = sorted(data.select(\"ASOFDATE\").distinct().rdd.map(lambda x : x[0]).collect())\n",
    "    indexed_asofDates = [ [i[0],i[1]] for i in zip(range(len(asofDates)), asofDates)]\n",
    "    indexed_asofDates_Frame = sc.parallelize(indexed_asofDates).toDF([\"index_date\",\"ASOFDATE2\"])\n",
    "    indexed_asofDates_Frame\n",
    "    indexed_data = data.join(indexed_asofDates_Frame, data.ASOFDATE == indexed_asofDates_Frame.ASOFDATE2)\n",
    "    indexed_data = indexed_data.drop(\"ASOFDATE2\")\n",
    "    indexed_data_copy = indexed_data\n",
    "    columns_renamed = [col+'1' for col in indexed_data.columns]\n",
    "    indexed_data_copy = indexed_data_copy.toDF(*columns_renamed)\n",
    "    \n",
    "    \n",
    "    day_to_day_risk = indexed_data.join(indexed_data_copy, \n",
    "                                             # need versioned data\n",
    "                                             #(indexed_data.START_DATE == indexed_data_copy.START_DATE1)\n",
    "                                             #(indexed_data.KEY == indexed_data_copy.KEY1)&\\\n",
    "                                             (indexed_data.index_date == indexed_data_copy.index_date1 - 1)&\\\n",
    "                                             (indexed_data_copy.MINIMUM_VARIATION1.isNull()))\n",
    "    drop_list = ['KEY1', 'ASOFDATE1','MATURITY1','index_date1','START_DATE1','MINIMUM_VARIATION']\n",
    "\n",
    "    \n",
    "    for mat in maturities_list:\n",
    "        day_to_day_risk = day_to_day_risk.withColumn(\"variation_\"+mat,day_variation(mat,mat+'1'))\n",
    "\n",
    "    day_to_day_risk = day_to_day_risk.select([column for column in day_to_day_risk.columns if column not in indexed_data_copy.columns])\n",
    "\n",
    "    day_to_day_risk.show()\n",
    "    \n",
    "    return day_to_day_risk\n",
    "\n",
    "data_with_variation = day_to_day_variation(pivoted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+----------+------------+------------+------------+------------+---------------+---------------+---------------+-----+--------------------+\n",
      "|  ASOFDATE|KEY|         START_DATE|MINIMUM_VARIATION| 1Y| 2M| 2Y| 3M|DEC17|MAR18|SEP18|index_date|variation_1Y|variation_2M|variation_2Y|variation_3M|variation_DEC17|variation_MAR18|variation_SEP18|label|            features|\n",
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+----------+------------+------------+------------+------------+---------------+---------------+---------------+-----+--------------------+\n",
      "|27/12/2016| 74|27/12/2016 12:43 PM|             62.0|  0| 80|  0|  0|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[1,11],[80.0,...|\n",
      "|27/12/2016| 74|27/12/2016 12:43 PM|             62.0|  0| 80|  0|  0|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[1,11],[80.0,...|\n",
      "|27/12/2016| 74|27/12/2016 12:43 PM|             62.0|  0| 80|  0|  0|    0|    0|    0|        26|         0.0|         0.0|        -1.0|         0.0|            0.0|            0.0|            0.0|    1|(14,[1,9],[80.0,-...|\n",
      "|27/12/2016| 77|27/12/2016 06:40 PM|              4.0|  0| 40|  0|  0|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[1,11],[40.0,...|\n",
      "|27/12/2016| 77|27/12/2016 06:40 PM|              4.0|  0| 40|  0|  0|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[1,11],[40.0,...|\n",
      "|27/12/2016| 77|27/12/2016 06:40 PM|              4.0|  0| 40|  0|  0|    0|    0|    0|        26|         0.0|         0.0|        -1.0|         0.0|            0.0|            0.0|            0.0|    1|(14,[1,9],[40.0,-...|\n",
      "|27/12/2016| 30|27/12/2016 08:22 AM|             43.0|  0|  0|  0| 10|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[3,11],[10.0,...|\n",
      "|27/12/2016| 30|27/12/2016 08:22 AM|             43.0|  0|  0|  0| 10|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[3,11],[10.0,...|\n",
      "|27/12/2016| 30|27/12/2016 08:22 AM|             43.0|  0|  0|  0| 10|    0|    0|    0|        26|         0.0|         0.0|        -1.0|         0.0|            0.0|            0.0|            0.0|    1|(14,[3,9],[10.0,-...|\n",
      "|27/12/2016| 56|27/12/2016 11:58 AM|             47.0|  0| 55|  0|  0|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[1,11],[55.0,...|\n",
      "|27/12/2016| 56|27/12/2016 11:58 AM|             47.0|  0| 55|  0|  0|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[1,11],[55.0,...|\n",
      "|27/12/2016| 56|27/12/2016 11:58 AM|             47.0|  0| 55|  0|  0|    0|    0|    0|        26|         0.0|         0.0|        -1.0|         0.0|            0.0|            0.0|            0.0|    1|(14,[1,9],[55.0,-...|\n",
      "|27/12/2016| 28|27/12/2016 07:44 PM|             14.0| 17|  0|  0|  0|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[0,11],[17.0,...|\n",
      "|27/12/2016| 28|27/12/2016 07:44 PM|             14.0| 17|  0|  0|  0|    0|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|           -1.0|            0.0|            0.0|    1|(14,[0,11],[17.0,...|\n",
      "|27/12/2016| 28|27/12/2016 07:44 PM|             14.0| 17|  0|  0|  0|    0|    0|    0|        26|         0.0|         0.0|        -1.0|         0.0|            0.0|            0.0|            0.0|    1|(14,[0,9],[17.0,-...|\n",
      "|27/12/2016| 48|27/12/2016 12:06 PM|             67.0|  0|  0|  0|  0|   36|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|     -0.5862069|            0.0|            0.0|    1|(14,[4,11],[36.0,...|\n",
      "|27/12/2016| 48|27/12/2016 12:06 PM|             67.0|  0|  0|  0|  0|   36|    0|    0|        26|         0.0|         0.0|         0.0|         0.0|     0.33333334|            0.0|            0.0|    1|(14,[4,11],[36.0,...|\n",
      "|27/12/2016| 48|27/12/2016 12:06 PM|             67.0|  0|  0|  0|  0|   36|    0|    0|        26|         0.0|         0.0|        -1.0|         0.0|            0.0|            0.0|            0.0|    1|(14,[4,9],[36.0,-...|\n",
      "|20/12/2016| 36|20/12/2016 03:28 PM|             40.0|  0|  0|  0|  0|    0|    0|   78|        19|         0.0|        -1.0|         0.0|         0.0|            0.0|            0.0|            0.0|    1|(14,[6,8],[78.0,-...|\n",
      "|20/12/2016| 36|20/12/2016 03:28 PM|             40.0|  0|  0|  0|  0|    0|    0|   78|        19|         0.0|         0.0|        -1.0|         0.0|            0.0|            0.0|            0.0|    1|(14,[6,9],[78.0,-...|\n",
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+----------+------------+------------+------------+------------+---------------+---------------+---------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Input: data  : données enrechies\n",
    "       threshold: seuil pour labélisé les données\n",
    "Output: labélise les données intermédiaire ayant une variation (par rapport au dernier résultat de la journée)\n",
    "        supérieur à -threshold- comme anomalies\n",
    "\"\"\"\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def labelize_data(data,threshold):\n",
    "\n",
    "    drop_columns = [\"ASOFDATE\",\"KEY\",\"START_DATE\",\"MINIMUM_VARIATION\",\"label\",\"index_date\"]\n",
    "    labeled_data = data.withColumn(\"label\",func.when(data.MINIMUM_VARIATION > threshold, 1).otherwise(0))\n",
    "    inputColumns =  [m for m in labeled_data.columns if m not in drop_columns]\n",
    "    labeled_data = labeled_data.na.fill(0)\n",
    "    assembler = VectorAssembler(inputCols= inputColumns,outputCol=\"features\")\n",
    "    ready_for_ML = assembler.transform(labeled_data)\n",
    "    ready_for_ML.show()\n",
    "    return ready_for_ML\n",
    "\n",
    "labeled_data = labelize_data(data_with_variation,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input : data: données prête pour la partie machine learning\n",
    "        date: date de séparation entre le train et le test\n",
    "Output: modéle de classification des anomalies (randomForest)\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def data_classifier(data,date):\n",
    "    labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "    trainingData = data.filter(data.ASOFDATE < date)\n",
    "    testData = data.filter(data.ASOFDATE >= date)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "    rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=3)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "    labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                                   labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "    pipeline = Pipeline(stages=[labelIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "    return model,predictions,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model,prediction,evaluator =  data_classifier(labeled_data,\"20/12/2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+----------+------------+------------+------------+------------+---------------+---------------+---------------+-----+--------------------+------------+-------------+--------------------+----------+--------------+\n",
      "|  ASOFDATE|KEY|         START_DATE|MINIMUM_VARIATION| 1Y| 2M| 2Y| 3M|DEC17|MAR18|SEP18|index_date|variation_1Y|variation_2M|variation_2Y|variation_3M|variation_DEC17|variation_MAR18|variation_SEP18|label|            features|indexedLabel|rawPrediction|         probability|prediction|predictedLabel|\n",
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+----------+------------+------------+------------+------------+---------------+---------------+---------------+-----+--------------------+------------+-------------+--------------------+----------+--------------+\n",
      "|20/12/2016| 71|20/12/2016 08:47 AM|              0.0|  0| 52|  0|  0|    0|    0|    0|        19|         0.0|         0.0|         0.0|         0.0|            0.0|           -1.0|            0.0|    0|(14,[1,12],[52.0,...|         1.0|    [1.0,2.0]|[0.33333333333333...|       1.0|             0|\n",
      "|20/12/2016| 50|20/12/2016 05:07 AM|              0.0|  0|  0|  0|  0|    0|    0|   66|        19|         0.0|         0.0|         0.0|         0.0|            0.0|           -1.0|            0.0|    0|(14,[6,12],[66.0,...|         1.0|    [1.0,2.0]|[0.33333333333333...|       1.0|             0|\n",
      "+----------+---+-------------------+-----------------+---+---+---+---+-----+-----+-----+----------+------------+------------+------------+------------+---------------+---------------+---------------+-----+--------------------+------------+-------------+--------------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionction.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
